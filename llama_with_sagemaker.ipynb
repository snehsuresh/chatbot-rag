{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0a4337",
   "metadata": {},
   "source": [
    "# Deploys llama model using huggingface and sagemaker\n",
    "## Runs in aws sagemaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3885b4-1399-481b-a305-687a48619245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import numpy\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84123b82-de3f-4617-855f-3482177e9b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_execution_role = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b89cf-3aab-46bd-af11-8ba3ab5b7053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_sagemaker_session(default_bucket=None):\n",
    "    global sagemaker_execution_role\n",
    "    session = sagemaker.Session(default_bucket=default_bucket)\n",
    "    try:\n",
    "        sagemaker_execution_role = sagemaker.get_execution_role()\n",
    "    except ValueError:\n",
    "        iam = boto3.client(\"iam\")\n",
    "        role = \"sagemaker_execution_role\"\n",
    "        sagemaker_execution_role = iam.get_role(RoleName=role)[\"Role\"][\"Arn\"]\n",
    "    return session, sagemaker_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ccc8f-cada-4c94-a564-d058c4663ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_account_id(account_id):\n",
    "    return \"*\" * len(account_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b7ac7-6c66-46f9-aa77-4fb2d997fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    sagemaker_session_bucket = None\n",
    "    session, sagemaker_execution_role = setup_sagemaker_session(default_bucket=sagemaker_session_bucket)\n",
    "    account_id = sagemaker_execution_role.split(\":\")[4]\n",
    "    masked_account_id = mask_account_id(account_id)\n",
    "    masked_role = sagemaker_execution_role.replace(account_id, masked_account_id)\n",
    "    print(f\"sagemaker role ARN : {masked_role}\")\n",
    "    print(f\"sagemaker session region : {session.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c93eb2-4a5e-4c63-8edc-ca00d586c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3459bbf7-c5d0-41ea-b819-4b346b79a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10373702-5b60-4368-862f-50ff5967be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.0.3\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a8abf4-4f97-4804-b9ec-9ec2ced0fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "number_of_gpu = 4\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"meta-llama/Llama-2-7b-hf\", # model_id from hf.co/models. we are using llama2 7b\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text)\n",
    "  # 'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=sagemaker_execution_role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7eb7da-f64a-45df-87ea-f7bca3bbc782",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2748bb82-49ff-4f11-a23d-e9ead8a38689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  # volume_size=400, # If using an instance with local SSD storage, volume_size must be None, e.g. p4 but not p3\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247e890-0974-4957-a582-bcb78febec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = llm.predict({\n",
    "    \"inputs\": \"\"\"<|prompter|>What are some cool ideas to do in the summer?<|endoftext|><|assistant|>\"\"\"\n",
    "})\n",
    "\n",
    "print(chat[0][\"generated_text\"])\n",
    "#     <|prompter|>What are some cool ideas to do in the summer?<|endoftext|><|assistant|>There are many fun and exciting things you can do in the summer. Here are some ideas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ac50b",
   "metadata": {},
   "source": [
    "## # Now we will show how to use generation parameters in the parameters attribute of the payload. In addition to setting custom temperature, top_p, etc, we also stop generation after the turn of the bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b1c6ba-a0da-4972-897f-8fb17f44fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define payload\n",
    "prompt=\"\"\"<|prompter|>How can i stay more active during winter? Give me 3 tips.<|endoftext|><|assistant|>\"\"\"\n",
    "\n",
    "# hyperparameters for llm\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.7,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"<|endoftext|>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "# send request to endpoint\n",
    "response = llm.predict(payload)\n",
    "\n",
    "# print(response[0][\"generated_text\"][:-len(\"<human>:\")])\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4da0d-0c3e-44e8-88c0-ae45c27a120f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
